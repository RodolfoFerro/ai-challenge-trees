# Generative Adversarial Networks
## Task # 2

### Reading route1
1. DCGANs - Radford et al. (2015)
 Shows how convolutional layers can be used with GANs and provides a
 series of additional architectural guidelines for doing this.

2. Improved Techniques for Training GANs -  Salimans et al. (2016)
 Provide a series of recommendations for building on the architectural
 guidelines laid out in the DCGAN paper above.

3. Generative Adversarial Networks - Goodfellow et al. (2014)
 The original paper, in this GANs framework is defined

4. CycleGAN - Zhu et al. (2017)
 One of the most elegant loss function formulations I could read 

5. Wasserstein GAN - Arjovsky et al. (2017) (optional)
 Do cost functions matter in the GAN training?

6. The Numerics of GANs - Mescheder et al. (2018) (optional)
 GANs are broken at both the computational and algorithmic levels.


### Aditional resources
* [Goodfellow NIPS 2016 tutorial](https://youtu.be/HGYYEUSm-0Q)